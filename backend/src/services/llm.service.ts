import OpenAI from 'openai';

/**
 * LLM è°ƒç”¨æœåŠ¡
 * è´Ÿè´£è°ƒç”¨ OpenAI API å¹¶å¤„ç†å“åº”
 */
class LLMService {
  private client: OpenAI;
  private model: string;
  private maxTokens: number;
  private timeout: number = 30000; // 30 ç§’è¶…æ—¶

  constructor() {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY is not set in environment variables');
    }

    this.client = new OpenAI({
      apiKey,
      timeout: this.timeout
    });

    this.model = process.env.OPENAI_MODEL || 'gpt-4o-mini';
    this.maxTokens = parseInt(process.env.OPENAI_MAX_TOKENS || '4000');
  }

  /**
   * è°ƒç”¨ LLM API
   * @param prompt ç»„è£…å¥½çš„ Prompt
   * @param options å¯é€‰é…ç½®
   * @returns LLM å“åº”å†…å®¹
   */
  async callLLM(
    prompt: string,
    options?: {
      temperature?: number;
      maxRetries?: number;
    }
  ): Promise<string> {
    const temperature = options?.temperature ?? 0.7;
    const maxRetries = options?.maxRetries ?? 1;

    let lastError: Error | null = null;

    for (let attempt = 0; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸ¤– Calling LLM (attempt ${attempt + 1}/${maxRetries + 1})...`);

        const response = await this.client.chat.completions.create({
          model: this.model,
          messages: [
            {
              role: 'user',
              content: prompt
            }
          ],
          temperature,
          max_tokens: this.maxTokens
        });

        const content = response.choices[0]?.message?.content;

        if (!content) {
          throw new Error('LLM returned empty response');
        }

        console.log(`âœ… LLM call successful`);
        return content;
      } catch (error) {
        lastError = error as Error;
        console.error(`âŒ LLM call failed (attempt ${attempt + 1}):`, error);

        // å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
        if (attempt < maxRetries) {
          await this.sleep(1000 * (attempt + 1)); // é€’å¢ç­‰å¾…æ—¶é—´
        }
      }
    }

    // æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    throw new Error(`LLM call failed after ${maxRetries + 1} attempts: ${lastError?.message}`);
  }

  /**
   * è§£æ JSON å“åº”
   * @param content LLM è¿”å›çš„å†…å®¹
   * @returns è§£æåçš„ JSON å¯¹è±¡
   */
  parseJSONResponse(content: string): any {
    try {
      // å°è¯•æå– JSON ä»£ç å—
      const jsonMatch = content.match(/```json\s*([\s\S]*?)\s*```/);
      if (jsonMatch) {
        return JSON.parse(jsonMatch[1]);
      }

      // å°è¯•ç›´æ¥è§£æ
      return JSON.parse(content);
    } catch (error) {
      console.error('Failed to parse JSON response:', content);
      throw new Error('LLM è¿”å›çš„å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„ JSON æ ¼å¼');
    }
  }

  /**
   * ç¡çœ å‡½æ•°
   */
  private sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * è·å–å½“å‰é…ç½®
   */
  getConfig() {
    return {
      model: this.model,
      maxTokens: this.maxTokens,
      timeout: this.timeout
    };
  }
}

// å¯¼å‡ºå•ä¾‹
export const llmService = new LLMService();
